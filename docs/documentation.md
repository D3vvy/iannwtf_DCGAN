# Deep Convolutional Generative Adversarial Model

## Introduction: What is a DCGAN

Generative Adversarial Models are fairly new branch of unsupervised learning. Their goal is to generate samples (images, text, music, videos) which resembles their trainings-set. The remarkable thing is: It does **not reproduce** them; i.e. it generates images which were never there before.

To dive further into the topic and our implementation of, let's cover the basics first. 

#### Basics 1: What is a Generativ Model
Like it was said before, generativ models are able to randomly generate observable data values. Let's assume a generative Model **g** is trained on a training data **X** sampled from a true distribution **D**. If, after the training is complete, we provide **g** with some standard random distribution **Z** it will produce a distribution **D'** which is supposed to (more or less) closely resemble **D**.
The convenient way to determine **g** involes a maximum likelihood estimation and further complex caculations.

#### Basics 2: What is a Generativ Adversarial Model
By using an adversarial training process we can have an elegent way of avoiding these convoluted calculations. The idea is that instead of having only a generating Model **g** called the **Generator**, we also bring a discriminating Model **d** in the playing field which is called the **Discriminator**. 
Assuming our training data **X** is sampled from a Distribution **D_d**, after training **g** is able map random input **D<sup>n</sup> ==> D<sup>d</sup>**. It will try to make those samples as "good" as realistic as possible. 
The discriminator **d**'s task is to map **D<sup>d</sup> ==> {0,1}**. It will get real samples from **X** and "fake" samples **X<sub>fake</sub>** generated by **g** and try to discriminate them by ideally labeling all samples **X** with 1 and all samples **X<sub>fake</sub>** with 0.
The reason for having these 2 models is to put them up against each other, i.e. **d** will try to "filter out" all generated sample **X<sub>fake</sub>** and recognize all real samples **X**. On the other hand **g** will try to create samples which closely resemble **X** so that **d** is not able to categorize them as fakes. 

###### Analogy
Imagine a criminal who is trying to counterfeit famous paintings to later sell them a museum in his town (**Generator**). The museums, on the other hand, have employed an art expert whose salary depends on deciding whether a painting is real or a fake (**Discriminator**. This is a *zero-sum game* setup, i.e. the better the criminal does, the less money the art expert gets; and the better the art expert does in distinguishing between real and fake potraits, the less money the criminal earn. By having this kind of competition, we can ensure both the criminal and the art expert will try their best. If the expert has found a solid way to discriminate, the criminal will change his style until they again pass at which point the expert will again try to improve him self.

###### Training
As described above, the training process is supposed to resemble a *zero-sum game*. ?So the loss functions have to be written in that manner, i.e. "contradictory"? 


# add: they are neural networks
# add: we train them in alternating manner
# add. loss functions

#### Basics 3: What is a Deep Convolutional Generativ Adversarial Model Model




1. Explain: What is a GAN
    - concept (using an Analogy)
    - theoretical basis
        + euqations...
1,2 What do we try to do

??? Related Work and Simlar Approaches

1,4 Preprocessing

1,5. Network Structure and Design Choices

2. Used Procedures/ Tricks / ...
    - Dropout
    - 

10 Evaluation and Comparison



# Task Description

# Related Work and Similar Approaches

# Theoretical Basis and used Procedures

# Network Structure and Desgin Choices

# Performance Evaluation and Comparison

