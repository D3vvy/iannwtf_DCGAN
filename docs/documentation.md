# Deep Convolutional Generative Adversarial Model

## Introduction: What is a DCGAN
Generative Adversarial Models are fairly new branch of unsupervised learning. Their goal is to generate samples (images, text, music, videos) which resembles their trainings-set. The remarkable thing is: It does **not reproduce** them; i.e. it generates images which were never there before.

To dive further into the topic and our implementation of, let's cover the basics first.

#### Basics 1: What is a Generative Model
Like it was said before, generativ models are able to randomly generate observable data values. Let's assume a generative Model **g** is trained on a training data **X** sampled from a true distribution **D**. If, after the training is complete, we provide **g** with some standard random distribution **Z** it will produce a distribution **D'** which is supposed to (more or less) closely resemble **D**.
The convenient way to determine **g** involes a maximum likelihood estimation and further complex caculations.

#### Basics 2: What is a Generative Adversarial Model
By using an adversarial training process we can have an elegent way of avoiding these convoluted calculations. The idea is that instead of having only a generating Model **g** called the **Generator**, we also bring a discriminating Model **d** in the playing field which is called the **Discriminator**.
Assuming our training data **X** is sampled from a Distribution **D<sup>d</sup>**, after training **g** is able map random input **D<sup>n</sup> ==> D<sup>d</sup>**. It will try to make those samples as "good" as realistic as possible.
The discriminator **d**'s task is to map **D<sup>d</sup> ==> {0,1}**. It will get real samples from **X** and "fake" samples **X<sub>fake</sub>** generated by **g** and try to discriminate them by ideally labeling all samples **X** with 1 and all samples **X<sub>fake</sub>** with 0.
The reason for having these 2 models is to put them up against each other, i.e. **d** will try to "filter out" all generated sample **X<sub>fake</sub>** and recognize all real samples **X**. On the other hand **g** will try to create samples which closely resemble **X** so that **d** is not able to categorize them as fakes.

###### Analogy
Imagine a criminal (**Generator**) who is trying to counterfeit famous paintings to later sell them to a museum in his town. The museum on the other hand, has employed an famous art expert (**Discriminator**) whose salary depends on deciding whether a painting is real or a fake. This is a *zero-sum game* setup, i.e. the better the criminal does, the less money the art expert gets; and the better the art expert does in distinguishing between real and fake potraits, the less money the criminal earn. By having this kind of competition, we can ensure both the criminal and the art expert will try their hardest. Even if the expert has found a solid way to discriminate fake and genuine pictures, the criminal will change his style until they again pass, at which point the expert will again try to improve himself and so on ...
The only real difference to the real training process is that both Discriminator and Generator are untrained at the start.

###### Theoretical Basis of the Training
HIER ALLGEMEINT HALTEN; NICHT UNSERS BESCHREIBEN
As described above, the training process is supposed to resemble a *zero-sum game*. ?So the loss functions have to be written in that manner, i.e. "contradictory"?


# add: they are neural networks
# add: we train them in alternating manner
# add. loss functions

#### Basics 3: What is a Deep Convolutional Generative Adversarial Model Model




1. Explain: What is a GAN
    - concept (using an Analogy)
    - theoretical basis
        + equations...
1,2 What do we try to do


??? Related Work and Similar Approaches

1,4 Preprocessing

## Network Structure and Design Choices
Our network architecture is a scaled down version inspired by [Radford et al. (2015)](https://arxiv.org/abs/1511.06434). We do not have such a powerful architecture since our data has lower dimensionality (64x64, grayscale not rgb). Additionally, CHANGEX: Training GAN is really hard since they are XXX.
The generator first expands the input z_vector of size 100 to a vector of length 512x4x4 and reshapes this vector into a 512x4x4 matrix. It then expands the 4x4 feature maps via four [transposed convolutions](https://arxiv.org/abs/1603.07285) with stride 2, a 5x5 kernel size and decreasing amount of feature maps to a single 64x64 matrix of depth 1 (grayscale), i.e. the generated fake image. Except for the final layer, we use batch-normalization after each convolution.
The discriminator mirrors this architecture by convoluting its input four times, reshaping to a 1d-layer and mapping to a single output unit. We're using strides of 2 and 5x5 kernels again. To regulate the capacity with respect to the generator's training progress, we apply dropout after the second and fourth convolution. No batch-normalization is applied in the discriminator.

## Training Procedure
During training, we update the discriminator with mini-batches consisting of 64 randomly picked real images and 64 z_vectors. The generator is updated using the same z_vector batch as well as an additional 64 z_vectors. We use the Adam optimizer with initial learning rates of 0.0002 for the discriminator and 0.0002 for the generator.
The two learning rates are adjusted given the discriminator's accuracy on the fake as well as the real data. We set two thresholds to determine the learning rate adjustment:[changex picture of interval] if either of the discriminator's accuracies is below the first (lower) threshold the discriminator's learning rate is increased slightly and the generator's decreased since the discriminator's classification is considered less informative. If the discriminator's accuracies reach above the second (higher) threshold, we do the opposite, since the discriminator should not get too powerful and the generator gets very informative feedback. In between the thresholds we increase both learning rates, because we can. This enables us to control the training process which again yields better results.
The key to success in training a DCGAN is tuning the interplay between generator and discriminator, because one of them gets "ahead". If the discriminator is too powerful, it will classify all the generators image as fake. In the opposite case, the discriminator is fooled every time and hence classifies everything as true. For the generator both cases result in the same situation: it is left with no useful feedback. Both architecture and learning parameters have to be tuned to achieve good results.
The game between discriminator and generator was hard to balance. We were not able to fully eliminate the chance of the network getting "stuck". Our first approach to avoid oscillations was to [leave either the discriminator-update or the generator-update out](http://torch.ch/blog/2015/11/13/gan.html) for one training batch if the accuracies behaved as described above. The interval needs very precise tuning: setting the lower threshold too low results in the discriminator failing and setting the higher threshold too high makes it too powerful. During training, the network eventually "recovers", but a lot of mini-batches are wasted. To still be able to use all batches for both components of our GAN, we instead decided to increase/decrease the learning rate to this update schedule. Limiting the learning rate within a range was required, again to avoid oscillations. The procedure yielded more stable and consistent results and visually clear transitions between sample images (lower learning rate at times).

10 Evaluation and Comparison



## Evaluation

### Z-Paramter Change
![alt text](https://github.com/D3vvy/iannwtf_DCGAN/blob/master/images-gifs/showcase_Z-Change1.png "Z_Change1")
![alt text](https://github.com/D3vvy/iannwtf_DCGAN/blob/master/images-gifs/showcase_Z-Change2.png "Z_Change2")

One question which intrigued us while building the Network was how the different paramters of the Z-vector influence the resulting images. To get some insight into it, we created a Z-vector and iterated over different values for only one of its 100 paramters while keeping the other 99 paramters. At the end we have 12 of those images (of cause it is possible to change in this code). In the visualisations above you can see in total 8/12 of those images (oteration is done in the rows while columns are indicating different values for one parameter).

What we can see here is firstly, that the value of every parameter influences the background if it is big enough, which makes sense. 

In the first row of the first sample image it is also recognizable that this paramter influences the gender of the generated face, the higher this value gets (more to the right) the more masculin the face gets (although the hair still is feminine).

In the fourth row of the first sample we can see that this paramter also influence the gesture, since the person on the left is clearly laughing (teeth visible) while the highest value (most right picture) is just smiling.

### Z-Interpolation
![alt text](https://github.com/D3vvy/iannwtf_DCGAN/blob/master/images-gifs/showcase_Z-interpolation.png "Z_Interpolation")
![alt text](https://github.com/D3vvy/iannwtf_DCGAN/blob/master/images-gifs/showcase_Z-interpolation-gif.gif "Z_Interpolation_Gif")

The idea for Z-interpolation was provided by [Generating Faces with Deconvolution Networks](https://zo7.github.io/blog/2016/09/25/generating-faces.html). What we do is to take two random Z-vectors and over the course, of a total of 64 images, interpolate between them. The pictures and the give above are exactly those 64 vectors, after the Generator mapped them onto faces.

Why do we do this and how does it help use evaluate our model: A good model maps the given Z-Vectors into a space (in our case in a 64<sup>2</sup>dimensional), lets call it "face-space". A bad model may just try to reproduce the images and not generate new ones. By taking little steps, we can check that the transitions are smooth and thus, a good "face-space" was found.

In terms of **evaluation** we can say with a clear conscience that our model does a good job. This is due to the fact that the interpolating values from the one image (top-left of the left image) to another one (bottom-right of the left image) are faces them self. This shows that the mapping into a "face-space" was successful and images are genuinely generated and not duplicated.



### Look at change of pictures blalbal
![alt text](https://github.com/D3vvy/iannwtf_DCGAN/blob/master/images-gifs/showcase_stats.png "Stats")
1. Random Noise -> Discriminator Learns
2. Than Generator Learn
3. Equilibirum and mutual change

### Conclusion
Finally, one can say that Generative Adversarial Networks implemented via Deep Convolution are a powerful and versatile tool. Although in our case we only generated images, there is no real limit to what it can do. Be it [Video](http://web.mit.edu/vondrick/tinyvideo/), Sound or [Text](https://arxiv.org/abs/1605.05396); after adjustments in the architecture they all can be done. 
Altough our Network surely has room for improvement, the results are already convincing. There may be some samples which do not really look like a face, what indicates the Network mapped them out of the (changex: coordinatensystem der gesichter). On the other hand, most of them do not only resemble a face, but a hard to discriminate even for the human eye. That fact alone makes this project a success in our book.

# Task Description

# Related Work and Similar Approaches

# Theoretical Basis and used Procedures

# Network Structure and Desgin Choices

# Performance Evaluation and Comparison
